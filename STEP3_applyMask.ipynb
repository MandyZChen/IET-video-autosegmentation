{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script was first developed by Zhimin Chen and later maintained by Yitian Ma\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib tk\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage.transform\n",
    "import skimage.io\n",
    "import skimage.filters\n",
    "import skimage.util\n",
    "import glob\n",
    "import scipy.ndimage\n",
    "import math\n",
    "from skimage import color\n",
    "from numpy import linalg\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.widgets import Button\n",
    "\n",
    "# FIXME\n",
    "movie_name = '99StrangerOnBus' \n",
    "JPEG_DIR = 'FrameImages'\n",
    "ANNOTATIONS_DIR = 'Annotations'\n",
    "DICTIONARY_DIR = 'DictOutput'\n",
    "BLURRED_DIR = 'BluredImages'\n",
    "MOVIEDIR = 'MovieOutput'\n",
    "NEWMASKDIR = 'NewMask'\n",
    "CONTEXT_DIR = 'ContextOccluded'\n",
    "\n",
    "outdir = os.path.join(BLURRED_DIR, movie_name)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "if not os.path.exists(MOVIEDIR):\n",
    "    os.makedirs(MOVIEDIR)\n",
    "outdir = os.path.join(NEWMASKDIR, movie_name)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "outdir = os.path.join(BLURRED_DIR, movie_name, CONTEXT_DIR)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpr functions\n",
    "\n",
    "# Remove the duplicate member within the list\n",
    "# Return a list with unique members\n",
    "def Remove(duplicate):\n",
    "    final_list = []\n",
    "    for num in duplicate:\n",
    "        if num not in final_list:\n",
    "            final_list.append(num)\n",
    "    return final_list\n",
    "\n",
    "# Adjust the size of the image centered upon the center\n",
    "# The image is enlarged / shrinked based on the zoom_factor\n",
    "def zoom(image,center,zoom_factor):\n",
    "    s_shape = image.shape\n",
    "    image_resized = skimage.transform.rescale(image, zoom_factor, preserve_range = True)\n",
    "    l_shape = image_resized.shape\n",
    "    crop_width = []\n",
    "    up_crop = center[0]*abs(1-zoom_factor)\n",
    "    low_crop = (s_shape[0]-center[0])*abs(1-zoom_factor)\n",
    "    left_crop = center[1]*abs(1-zoom_factor)\n",
    "    right_crop = (s_shape[1]-center[1])*abs(1-zoom_factor)\n",
    "    crop_width.append((up_crop,low_crop))\n",
    "    crop_width.append((left_crop,right_crop))\n",
    "    \n",
    "    if zoom_factor >= 1:\n",
    "        cropped_image = skimage.util.crop(image_resized, crop_width)\n",
    "    else:\n",
    "        cropped_image = skimage.util.crop(image, crop_width)\n",
    "    return cropped_image\n",
    "\n",
    "# Apply the blur effect on the mask\n",
    "# Adjust the simga to sharpify the blurring effect\n",
    "def applyBlur(image, mask, sigma=50.0, feather=20):\n",
    "    image = skimage.util.img_as_float(image)\n",
    "    image = skimage.transform.resize(image,mask.shape, preserve_range = True)\n",
    "    blurred = skimage.filters.gaussian(image, sigma=sigma, multichannel = True) \n",
    "    blurredmask = skimage.filters.gaussian(skimage.util.img_as_float(mask), sigma=feather, multichannel=True)\n",
    "    blurredmask = np.expand_dims(blurredmask, 2)\n",
    "    inverted_mask =np.ones([blurredmask.shape[0],blurredmask.shape[1],1])-blurredmask\n",
    "    blended = blurredmask*blurred + inverted_mask*image\n",
    "    return blended\n",
    "\n",
    "def applyBlurContext(image, mask, sigma=50.0, feather=30):\n",
    "    image = skimage.util.img_as_float(image)\n",
    "    image = skimage.transform.resize(image,mask.shape, preserve_range = True)\n",
    "    blurred = skimage.filters.gaussian(image, sigma=sigma, multichannel=True)\n",
    "    blurredmask = skimage.filters.gaussian(skimage.util.img_as_float(mask), sigma=feather, multichannel=True)\n",
    "    blurredmask = np.expand_dims(blurredmask, 2)\n",
    "    inverted_mask =np.ones([blurredmask.shape[0],blurredmask.shape[1],1])- blurredmask\n",
    "    blended = inverted_mask*blurred + blurredmask*image\n",
    "    return blended\n",
    "\n",
    "# Check whether two masks are similar or not.\n",
    "# Similarity = intersection( Mask1, Mask2 ) / union( Mask1, Mask2 ).\n",
    "# The two masks are similar if the similarity level is above the predefined threshold.\n",
    "def hasSimilarity(pre_mask, mask, IOU_THRESHOLD = 0.6):\n",
    "    union_area = np.sum((pre_mask+mask)>0) #union area\n",
    "    join_area = np.sum((pre_mask+mask)>1) #union area\n",
    "    if (float(join_area)/float(union_area) > IOU_THRESHOLD):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Flatten the nested array.\n",
    "# This is used to handle conditions where the user provided data ( image data ) is somehow nested within a dummy array.\n",
    "# e.g. flatte( [[ A, B]] ) -> [A, B]\n",
    "def flatten( array, needToFlattenTwice=False ):\n",
    "    if needToFlattenTwice:\n",
    "        if len(array) == 1:\n",
    "            array = array[0]\n",
    "    else:\n",
    "        if len(array.shape) == 3:\n",
    "            return array\n",
    "        if len(array) == 1:\n",
    "            array = array[0]\n",
    "    return array\n",
    "\n",
    "# Check if the detected objects can be categorized as human.\n",
    "# If the object has a score higher than the human_threshold, it is categoried as human.\n",
    "# Returns two variables [ shouldCheck, num_target_directory ]\n",
    "# shouldCheck = There are human within the frame and the program shoul process / check the frame\n",
    "# num_target_category = The number of human within the frame\n",
    "def satisfyThreshold(scores, classes, human_threshold):\n",
    "    idx = 0\n",
    "    shouldCheck = False\n",
    "    num_target_category = 0\n",
    "    while (idx < len( scores )):\n",
    "        if (scores[idx] > human_threshold and classes[idx] == 1):\n",
    "            shouldCheck = True;\n",
    "            num_target_category += 1\n",
    "        idx = idx + 1;\n",
    "    return shouldCheck, num_target_category\n",
    "\n",
    "# Check the degree of overlapping of two rectangles\n",
    "# It is used to check whether the target characters in two consecutive frames \n",
    "# are the same so that we can semi-autonomously track the target character as long as it stays relatively still\n",
    "def hasSimilarityBox(box1, box2, threshold = 0.7):\n",
    "    left_1_x = box1[1]\n",
    "    top_1_y = box1[0]\n",
    "    right_1_x = box1[3]\n",
    "    bot_1_y = box1[2]\n",
    "    \n",
    "    left_2_x = box2[1]\n",
    "    top_2_y = box2[0]\n",
    "    right_2_x = box2[3]\n",
    "    bot_2_y = box2[2]\n",
    "    \n",
    "    overlap_x = max(0, min(right_1_x, right_2_x) - max(left_1_x, left_2_x))\n",
    "    overlap_y = max(0, min(bot_1_y, bot_2_y) - max(top_1_y, top_2_y))\n",
    "\n",
    "    box1_area = (right_1_x - left_1_x) * (bot_1_y - top_1_y)\n",
    "    box2_area = (right_2_x - left_2_x) * (bot_2_y - top_2_y)\n",
    "    overlap_area = overlap_x * overlap_y\n",
    "    union_area = box1_area + box2_area - overlap_area\n",
    "    \n",
    "    if (union_area == 0):\n",
    "        return False\n",
    "    \n",
    "    percentage = overlap_area / float(union_area)\n",
    "    assert percentage >= 0 and percentage <= 1\n",
    "    \n",
    "    return percentage >= threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of file names\n",
    "image_list = sorted(glob.glob(os.path.join(ANNOTATIONS_DIR, movie_name,'*.jpg')))\n",
    "dict_list =  sorted(glob.glob(os.path.join(DICTIONARY_DIR, movie_name,'*.npz')))\n",
    "frame_list =  sorted(glob.glob(os.path.join(JPEG_DIR, movie_name,'*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClipInfo class\n",
    "# The video is broken down into multiple clips\n",
    "# For each clip, user should specify wether they want the program to blur the clip or just simply skip it.\n",
    "# The logic behind it is that for some frames, the traget character is not there. \n",
    "# So it is unnecessary for the program to process those frames.\n",
    "# Note: finding the target character requires both user input ( click on the traget character box) and beefy amount of \n",
    "# computation resources as the program needs to iterate through all the detected masks, no matter those masks are actually \n",
    "# target characters or not.\n",
    "\n",
    "# Usage:\n",
    "# E.g. \n",
    "# To specify frames #30 to #40 dont require blurring, add the following line\n",
    "# clipInfo.addInfo( start_index=30, end_index=40, shouldSkip=True)\n",
    "class ClipInfo:\n",
    "    def __init__(self):\n",
    "        self.intervals = []\n",
    "        self.flags = []\n",
    "        self.num_intervals = 0\n",
    "    \n",
    "    # make sure the information entered is basically correct\n",
    "    def sanityCheck(self):\n",
    "        for i in range(self.num_intervals):\n",
    "            if i == 0:\n",
    "                assert self.intervals[i][0] == 0\n",
    "            else:\n",
    "                assert self.intervals[i][0] - self.intervals[i-1][1] == 1\n",
    "            assert self.intervals[i][1] >= self.intervals[i][0]\n",
    "    \n",
    "    def addInfo(self, start_index, end_index, shouldSkip):\n",
    "        self.intervals.append([start_index, end_index+1])\n",
    "        self.flags.append(shouldSkip)\n",
    "        self.num_intervals += 1\n",
    "    \n",
    "    # return start_index, end_index, flag\n",
    "    def getInfo(self, index):\n",
    "        if (index >= self.num_intervals):\n",
    "            return -1, -1, False\n",
    "        return self.intervals[index][0], self.intervals[index][1], self.flags[index];\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ctypes\n",
    "def process(clipInfo, startClipIndex, IOU_THRESHOLD=0.6, human_threshold=0.7):\n",
    "    \n",
    "    # define some constants\n",
    "    zoom_factor = 1.1\n",
    "    \n",
    "    # define some other stuffs\n",
    "    image_first = plt.imread(frame_list[0])\n",
    "    images_shape = image_first.shape\n",
    "    \n",
    "    # Process all the clips\n",
    "    for i in range(startClipIndex, clipInfo.num_intervals):\n",
    "        print(\"~~~~~~~~~Start working on clip #%d ~~~~~~~~~~~~\" % i)\n",
    "        # get info for this clip\n",
    "        start_index, end_index, macroSkipFlag = clipInfo.getInfo(i)\n",
    "        # define masks\n",
    "        pre_mask = 0\n",
    "        pre_unwanted_mask = 0\n",
    "        pre_box = [0,0,0,0]\n",
    "        # for all the frames within this clip\n",
    "        for image_id, image_path in enumerate(frame_list[start_index:end_index]):\n",
    "        \n",
    "            # Get all the images, mask, and annotations for the related image\n",
    "            # FIXME\n",
    "            # image_name extraction might differ based on how the image naming convention\n",
    "            image_name = image_path[-9:-4]\n",
    "            original_frame = plt.imread(image_path)\n",
    "            annotated_image = plt.imread(image_list[image_id+start_index])\n",
    "            annotated_dict = np.load(dict_list[image_id+start_index],encoding = 'latin1',fix_imports = True)\n",
    "            \n",
    "            # Get the masks and other related fields for the specific annotat\n",
    "            masks = flatten( annotated_dict['detection_masks'] )\n",
    "            boxes = flatten( annotated_dict['detection_boxes'], True )\n",
    "            classes = flatten( annotated_dict['detection_classes'] )\n",
    "            scores = flatten( annotated_dict['detection_scores'] )\n",
    "\n",
    "            # mask_selected stores all the wanted masks ( target character(s) )\n",
    "            mask_selected=[]\n",
    "            # unwanted_mask_selected stores all the unwanted masks. \n",
    "            # Those unwanted masks will be excluded from the blurring process.\n",
    "            unwanted_mask_selected = []\n",
    "            \n",
    "            box_selected = [0,0,0,0]\n",
    "            unwanted_box_selected = [0,0,0,0]\n",
    "            \n",
    "            skipFlag = macroSkipFlag\n",
    "        \n",
    "            # Set \"shouldCheck\" to true if the annotation of the given frame doesn't contain any human character\n",
    "            # Otherwise, set \"shouldCheck\" to false. \"num_target_category\" is the number of human characters \n",
    "            # captured by the annotation of the given frame.\n",
    "            shouldCheck, num_target_category = satisfyThreshold(scores, classes, human_threshold)\n",
    "            num_unwanted_images = 0\n",
    "            \n",
    "            # We examine the frame if and only if the frame is worth checking and \n",
    "            # the skipFlag ( our manual input from the clipInfo ) is set to False\n",
    "            if shouldCheck and (not skipFlag):\n",
    "                # check all the masks\n",
    "                for idx in range(len(masks)):\n",
    "                    # we are only interested in the masks that belong to human category\n",
    "                    if (classes[idx] != 1):\n",
    "                        continue;\n",
    "                    # we are not interested in low precision masks\n",
    "                    if (scores[idx] < human_threshold):\n",
    "                        break;\n",
    "\n",
    "                    mask = masks[idx,:,:]\n",
    "                    mask = skimage.transform.resize(mask,(images_shape[0],images_shape[1]), preserve_range = True)\n",
    "                    \n",
    "                    # If the image contains similar mask in the previous frame,\n",
    "                    # append the index the mask_selected.\n",
    "                    if (hasSimilarity(pre_mask, mask, IOU_THRESHOLD)):\n",
    "                        mask_selected.append(idx)\n",
    "                        # if the box_selectd has not been set a value\n",
    "                        if (all(x == 0 for x in box_selected)):\n",
    "                            box_selected = boxes[idx]\n",
    "                            \n",
    "                    # If the image contains unwanted mask, increment the num_wanted_images counter.\n",
    "                    if (hasSimilarity(pre_unwanted_mask, mask, IOU_THRESHOLD)):\n",
    "                        unwanted_mask_selected.append(idx)\n",
    "                        num_unwanted_images += 1\n",
    "                        \n",
    "                # Remove duplicate masks from both unwanted_mask_selected & mask_selected\n",
    "                unwanted_mask_selected = Remove(unwanted_mask_selected)\n",
    "                mask_selected = Remove(mask_selected)\n",
    "\n",
    "            # It only contains the unwanted images, skip it as there is nothing valuable\n",
    "            if (num_unwanted_images == num_target_category):\n",
    "                skipFlag = True\n",
    "\n",
    "            # The position of the targeted mask only moves a little bit, skip it as the figure is just moving\n",
    "            if (hasSimilarityBox(box_selected, pre_box)):\n",
    "                skipFlag = True\n",
    "            \n",
    "            # As the selected mask is not empty, we dont require user input.\n",
    "            # In other words, if the program is able to recognize the target character in this frame,\n",
    "            # no user input is necessary\n",
    "            if (mask_selected):\n",
    "                skipFlag = True\n",
    "            \n",
    "            # Ask for user input if and only if we should check the frame and the skipFlag is set to False\n",
    "            if shouldCheck and (not skipFlag):\n",
    "                # show the first frame on a gui\n",
    "                os.system('say \"click\"')\n",
    "\n",
    "                hasAddedWanted = False\n",
    "                hasAddedUnwanted = False\n",
    "                while not (hasAddedWanted and hasAddedUnwanted):\n",
    "                    user_input = input(\"press 'a' to add wanted character, press 'd' to add unwanted character: \")\n",
    "                    plt.imshow(annotated_image)    \n",
    "\n",
    "                    mouse_clicks=[]\n",
    "                    mouse_clicks = np.array(plt.ginput(-1, timeout=0))\n",
    "                    temp_mask_selected = []\n",
    "                    \n",
    "                    if len(mouse_clicks):\n",
    "                        # Find the masks that were selected \n",
    "                        for mouse in range(mouse_clicks.shape[0]):\n",
    "                            mouse_x = mouse_clicks[mouse,0]\n",
    "                            mouse_y = mouse_clicks[mouse,1]\n",
    "                            for idx in range(masks.shape[0]):\n",
    "                                # For those human masks that have scores high enough, \n",
    "                                # append the masks to the selected_masks.\n",
    "                                if (scores[idx] >= human_threshold and classes[idx] == 1):\n",
    "                                    mask = masks[idx,:,:]\n",
    "                                    if mask[mouse_y.astype(int),mouse_x.astype(int)]:\n",
    "                                        temp_mask_selected.append(idx)\n",
    "                                        \n",
    "                        # Categorize wanted and unwanted masks\n",
    "                        if (user_input == 'a'):\n",
    "                            mask_selected = Remove(temp_mask_selected)\n",
    "                        elif (user_input == 'd'):\n",
    "                            unwanted_mask_selected = Remove(temp_mask_selected)\n",
    "                            \n",
    "                    if (user_input == 'a'):\n",
    "                        hasAddedWanted = True;\n",
    "                    elif (user_input == 'd'):\n",
    "                        hasAddedUnwanted = True;\n",
    "                    plt.close()\n",
    "\n",
    "            # join all selected masks\n",
    "            if len(mask_selected):\n",
    "                new_mask = masks[mask_selected[0],:,:]\n",
    "                for ind in mask_selected[1:]:\n",
    "                    new_mask = np.logical_or(new_mask,masks[ind,:,:])\n",
    "                # Find the box the wanted masks are in\n",
    "                box_selected = boxes[mask_selected[0]]\n",
    "                center_selected = []\n",
    "                center_selected.append((box_selected[2]+box_selected[0])/2*new_mask.shape[0])\n",
    "                center_selected.append((box_selected[3]+box_selected[1])/2*new_mask.shape[1])\n",
    "                new_mask = zoom(new_mask, center_selected,zoom_factor)\n",
    "            else:\n",
    "                new_mask = np.zeros((images_shape[0],images_shape[1]))\n",
    "\n",
    "            # join all unwanted selected masks\n",
    "            if len(unwanted_mask_selected):\n",
    "                new_unwanted_mask = masks[unwanted_mask_selected[0],:,:]\n",
    "                for ind in unwanted_mask_selected[1:]:\n",
    "                    new_unwanted_mask = np.logical_or(new_unwanted_mask,masks[ind,:,:])   \n",
    "                # Find the box the unwanted masks are in\n",
    "                unwanted_box_selected = boxes[unwanted_mask_selected[0]]\n",
    "                center_selected = []\n",
    "                center_selected.append((unwanted_box_selected[2]+unwanted_box_selected[0])/2*new_unwanted_mask.shape[0])\n",
    "                center_selected.append((unwanted_box_selected[3]+unwanted_box_selected[1])/2*new_unwanted_mask.shape[1])\n",
    "                new_unwanted_mask = zoom(new_unwanted_mask, center_selected,zoom_factor)\n",
    "            else:\n",
    "                new_unwanted_mask = np.zeros((images_shape[0],images_shape[1]))\n",
    "\n",
    "            # Save wanted and unwanted masks\n",
    "            new_mask = skimage.transform.resize(new_mask,(images_shape[0],images_shape[1]), preserve_range = True)\n",
    "            new_unwanted_mask = skimage.transform.resize(new_unwanted_mask,(images_shape[0],images_shape[1]), preserve_range = True)\n",
    "            # Write the wanted masks to the file\n",
    "            np.savez_compressed(os.path.join(NEWMASKDIR, movie_name, image_name), new_mask) # save mask \n",
    "            \n",
    "            # Blur the frame based on the wanted masks\n",
    "            blurred_frame = applyBlur(original_frame, new_mask)\n",
    "            blurred_frame = skimage.transform.resize(blurred_frame,(images_shape[0],images_shape[1]), preserve_range = True)\n",
    "            # Save the blurred image for the latter pipeline \n",
    "            plt.imsave(os.path.join(BLURRED_DIR,movie_name, image_name + '.jpg'), blurred_frame)  \n",
    "\n",
    "            # Save context occluded blurred image\n",
    "            blurred_frame_context = applyBlurContext(original_frame, new_mask)\n",
    "            blurred_frame_context = skimage.transform.resize(blurred_frame_context,(images_shape[0],images_shape[1]), preserve_range = True)\n",
    "            # Save the blurred image for the latter pipeline \n",
    "            plt.imsave(os.path.join(BLURRED_DIR,movie_name,CONTEXT_DIR, image_name + '.jpg'), blurred_frame_context)  \n",
    "\n",
    "            # Report progress\n",
    "            if np.remainder(image_id,20)== 0:\n",
    "                clear_output()\n",
    "            print('Processing '+ 'image ' + str(image_id+start_index) + '/'+ str(end_index-1))\n",
    "\n",
    "            # set previous frame's wanted masks, unwanted masks, and boxes\n",
    "            pre_mask = new_mask\n",
    "            pre_unwanted_mask = new_unwanted_mask\n",
    "            pre_box = box_selected\n",
    "\n",
    "    os.system('say \"done\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 190/200\n",
      "Processing image 191/200\n",
      "Processing image 192/200\n",
      "Processing image 193/200\n",
      "Processing image 194/200\n",
      "Processing image 195/200\n",
      "Processing image 196/200\n",
      "Processing image 197/200\n",
      "Processing image 198/200\n",
      "Processing image 199/200\n",
      "Processing image 200/200\n"
     ]
    }
   ],
   "source": [
    "# input clips info\n",
    "clipInfo = ClipInfo()\n",
    "# True = skip\n",
    "# False = need click\n",
    "# clipInfo.addInfo(0, 50,False)\n",
    "# clipInfo.addInfo(51, 100,True)\n",
    "# clipInfo.addInfo(101, 150,False)\n",
    "clipInfo.addInfo(151, 200,True)\n",
    "process(clipInfo, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT REMOVE\n",
    "# Handle edge case\n",
    "# Properly process the frame within the gap between two consecutive clips\n",
    "blur =  sorted(glob.glob(os.path.join(BLURRED_DIR, movie_name,'*.jpg')))\n",
    "prev = -1\n",
    "missing = []\n",
    "for i in range(len(blur)):\n",
    "    index = int(blur[i][-9: -4])\n",
    "    if (index - prev != 1):\n",
    "        missing.append(index-1)\n",
    "    prev = index\n",
    "    \n",
    "import shutil\n",
    "\n",
    "for i in range(len(missing)):\n",
    "    image_name = frame_list[missing[i]][-9:-4]\n",
    "    pre_image_name = frame_list[missing[i]+1][-9:-4]\n",
    "    shutil.copyfile(os.path.join(BLURRED_DIR,movie_name, pre_image_name + '.jpg'), \\\n",
    "             os.path.join(BLURRED_DIR,movie_name, image_name + '.jpg'))\n",
    "    shutil.copyfile(os.path.join(BLURRED_DIR,movie_name, CONTEXT_DIR, pre_image_name + '.jpg'), \\\n",
    "             os.path.join(BLURRED_DIR,movie_name, CONTEXT_DIR, image_name + '.jpg'))\n",
    "    shutil.copyfile(os.path.join(NEWMASKDIR,movie_name, pre_image_name + '.npz'), \\\n",
    "             os.path.join(NEWMASKDIR,movie_name, image_name + '.npz'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
